---
marp: true
---
![bg left:40%](https://www.einfochips.com/blog/wp-content/uploads/2019/04/3-ways-ai-chatbots-can-transform-the-telecom-industry-featured.jpg)
# **딥러닝 기반의 한글 문장 생성 기법**
### 손성환 | 로앤컴퍼니
### 강승식 | 국민대학교
<br />
 2023468101 최강훈

---
# **목차**
## 1. 자연어 생성 모델의 역사
---
<!-- _header: 자연어 생성 모델의 역사 -->
# 통계적 언어 모델 (Statistical Language Model, SLM)
- 토큰의 순서 정보에 따른 확률 분포.
- 조건부 확률의 연쇄 법칙을 이용한 다음 단어의 출현 확률을 기반으로 생성.

---
<!-- _header: 자연어 생성 모델의 역사 -->
# 조건부 확률의 연쇄 법칙
## 두 확률 P(A), P(B)에 대하여
- ### P(B|A) = P(A,B) / P(A)
- ### P(A,B) = P(A)P(B|A)
- ### P(x1,x2,x3,...,xn) = P(x1)P(x2|x1)P(x3|x1,x2)...P(xn|x1,...,xn-1)

---
<!-- _header: 자연어 생성 모델의 역사 -->
## 카운트 기반의 접근
- 훈련 데이터에서 각 단어의 출현 빈도를 카운트
- P(is|An adorable little boy) = count(An adorable little boy is) / count(An adorable little boy)
## 희소성 문제 (Sparcity Problem)
- 훈련 데이터 중 'An adorable little boy is' 단어 시퀀스가 없다면 확률은 0이 됨.
- 'is'는 문법적으로 적합하고 충분히 사용할 수 있는 단어.

---
# N-gram 언어 모델
- 이전 단어 전체를 카운트하는 대신 N개의 단어만을 고려해서 확률을 산출.
- P(is|An adorable little boy) ≈ P(is|boy) (unigram, 1-gram)
## 한계점
- 희소성 문제의 완전 해결은 근본적으로 불가.
- 정확도와 모델 크기간의 Trade-off


---
<!-- _header: 자연어 생성 모델의 역사 -->
# 딥러닝 기법의 도입
## LSTM (Long Short-Term Memory)
- 장/단기 기억을 가능하계 설계한 신경망의 구조.

### LSTM의 구성 요소
- 셀 상태 (Cell State) : 과거 데이터의 정보를 유지하는 장기 메모리
- 입력 게이트 (Input Gate) : 입력된 정보를 얼마나 Cell State에 반영할지 결정
- 삭제 게이트 (Forget Gate) : 이전 정보를 얼마나 Cell State에 반영할지 결정
- 출력 게이트 (Output Gate) : Cell State의 정보를 얼마나 다음 노드에 전달할지 결정

---
<!-- _header: 자연어 생성 모델의 역사 -->
### LSTM의 구조

![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F999F603E5ACB86A005)

---
<!-- _header: 자연어 생성 모델의 역사 -->
## Seq2Seq (Sequence-to-Sequence)
<img width="40%" style="margin: 0 auto" src="https://wikidocs.net/images/page/24996/seq2seq%EB%AA%A8%EB%8D%B811.PNG" />
- 두개의 RNN을 각각 Encoder와 Decoder로써 Context를 이용하여 연결하여 입출력하는 구조.

### 한계
1. 고정된 크기의 벡테어 모든 정보를 압축하는 과정에서의 정보 손실.
2. 기울기 손실(Gradient Vanishing) 문제 발생 .

<!-- _footer: '[1] ㅁㄴㅇㅁㄴㅇ'-->
---
<!-- _header: 자연어 생성 모델의 역사 -->
## Attention
