---
marp: true
---

![bg right:30%](https://i0.wp.com/bdtechtalks.com/wp-content/uploads/2016/08/8384110298_b0bc7d6435_o.jpg?resize=696%2C522&ssl=1)

# **코난 LLM: 한국어 대규모 언어 모델**

### 양승현 • 도원철 • 오창민 • 김정태 | 코난테크놀로지

<br />
 2023468101 최강훈

---

# 코난 LLM

- (주)코난테크놀로지에서 제작한 한국어 대규모 언어 모델

- 13B / 41B 두가지 패러미터 크기의 모델.
  - '패러미터 크기는 크지 않게, 토큰은 많이'
  - 상대적으로 작은 패러미터 크기로 인해 **RTX 3090 X 2**를 이용해 모델을 구동할 수 있음.
  - GPT-4 : 1.7T, GPT-3 : 175B, GPT-3 Turbo : 20B (추정치), LLaMA3: 70B / 8B

---

# 개발배경

- 기업에서 상용 / 오픈소스 LLM을 활용하려는 과정에서 발생하는 문제점
  - 기업 내부 데이터의 외부 유출 우려
  - 학습과 추론늘 위한 GPU 구입 등의 과다한 비용
  - 모델의 한계로 인한 환각 문제
- 이를 해결하기 위해 코난 LLM을 개발
  - 기업 내부에 설치 및 운영하여 데이터 유출을 원천 차단
  - 적절한 비용과 최적의 성능으로 효율적 운영
  - 벡터 검색을 통한 답변의 증강과 근거 제시가 가능

---

# 모델 학습

- 코난 LLM 13B 모델은 **H100 X 8 서버 8대**를 이용해 **23일간** 사전 학습을 진행
- AIHUB, 위키피디아 등의 공개 데이터, 뉴스, Github 등의 구입 데이터
- 코난 LLM 13B 모델의 학습 데이터 토큰수는 **5,395억**개
  - 유사한 파라미터 크기를 가진 Polyglot-ko 12.8B와 비교했을때 약 3배이상의 학습 데이터를 이용해 학습
  - 한국어 학습 토큰은 **3,318억**개로 GPT-3.5의 5.7억개, LLaMA 2의 12억개와 비교할 수 없을 만큼 많은 양을 학습

---

# 미세조정 및 최적화

- LLM은 한가지 태스크가 아닌 다양한 태스크로 학습할 수록 새 태스크에서 좋은 성능을 보인다고 알려져있음. <sup>[2]</sup> 코난 LLM은 하나의 태스크로 학습할 시 정확도가 84.2% 였지만, 다양한 태스크를 학습한 경우 93.9%의 정확도를 보임.
- 사전 학습 시 다중 GPU를 사용하기 위해 모델을 3D 병렬화함.
  - 텐서 병렬화
  - 파이프라인 병렬화
  - 데이터 병렬화
- 전체 미세조정시 발생하는 많은 자원과 비용을 줄이기 위해 **QLoRA** 기법을 사용해 미세조정.

<!-- _footer: '[2] Hyung Won Chung et al, Scaling Instruction-Finetuned Language Models ' -->

---

# QLoRA (Quantum Low-Rank Adaptation)

- LoRA란 모든 가중치를 미세 조정 하는 대신, 모든 행렬을 근사화하는 두개의 작은 행렬을 미세 조정하는 미세 조정 기법
- 양자화 : '모델 가중치와 활성화 함수 출력을 더 작은 비트 단위로 표현하도록 반환'하는 것.
  - LoRA : 8Bit 가중치 -> QLoRA : 4Bit 가중치
- 데이터의 정밀도를 줄이고, 연산에 필요한 용량을 감소
- 전체 미세조정 대비 GPU 메모리 사용을 4배 이상 절감

---

# 검색 증강 생성 (Retrival Augmented Generation, RAG)

## 간단히 말해...

1. 사용자의 질문을 검색어로 활용해 문서를 검색.
2. 검색된 문서의 내용을 모델에 컨텍스트로 주고 질문에 대한 답을 생성.

---

# 검색 증강 생성 (Retrival Augmented Generation, RAG)

- LLM의 답변에는 두가지 문제점이 있음.

  1. 사전학습 데이터 셋에 포함되어 있지 않은 정보는 답할 수 없음.
  2. 허위 정보를 답하는 할루시네이션 (Hallucination) 이 발생할 수 있음.

- 이를 해결하기 위해 추가 데이터 셋으로 미세 조정을 실행할 수 있지만, 실행 자체가 용이하지 않고, 최신 데이터가 변경 됨에 따라 빈번하게 재학습이 요구됨.

- 검색 증강 생성은 이를 해결하기 위한 효과적은 수단

---

# 사용 가능한 데이터 소스

- 문서
- API
- 데이터베이스
- 거의 모든 디지털 데이터

---

# 1. 지식 충돌

- 학습한 데이터의 한계로 인해 오답을 생성함.

```
Question: 현재 대한민국 대통령은 누구인가요?
Answer: 대한민국 대통령은 문재인 입니다.
```

- 컨텍스트를 기반으로 답을 생성하여 정답을 생성.

```
Context: 대한민국의  대통령은  대한민국의  국가원 수이자 행정부 수반이다.  현행 헌법에 따라 5년 단임제를 따른다.  현직 대통령은 2022년 5월 10일 취임한 제20대 윤석열 대통령이다.
Question:  현재 대한민국 대통령은 누구인가요?
Answer(Context  기반):  현재 대한민국 대통령은 윤석열입니다.  이는 문서에서 “2022년 5월 10일 취임한 제20대 윤석열 대통령이다.”라고 명시되어 있습니다
```

---

# 2. 자제력 있는 예측

- 사용자의 요구와 관련 없는 컨텍스트가 주어졌을 때, 모델은 컨텍스트를 기반으로 무작정 답을 하는 것이 아닌, 컨텍스트에서 단서를 찾을 수 없다는 답을 내놓을 수 있어야 함.

```
Question:  현재 대한민국 대통령은 누구인가요?
Context:  문재인은 1952년 1월 24일 경상남도 거제군 거제면 명진리 694-1번지에서 아버지 문용형과 어머니 진주 강씨 강한옥 사이에서 2남 3녀 중 장남으로 태어났다.
Answer: 문서에는  문재인  대통령의 출생지,  가족관계,  학력 등에 대한 정보가 있지만,  현재 대한민국 대통령이 누구인지에 대한 정보는 포함되어 있지 않습니다.
```

---

# 3. 권한 및 접근 제어를 통한 보안

- LLM을 사용하는 사용자에 따라 접근 가능한 문서를 제한함으로써 정보 보안을 확보.

### 정보 접근 권한 없음

```
Question:  코카콜라의 제조 방법이 무엇입니까?
Context:  코카콜라의 제조 방법은 극비이며, 이는 제한된 사용자에게만 제공됩니다.
Answer: 문서에 따르면 코카콜라의 제조 방법은 제한된 사용자에게만 제공되는 것으로 보입니다. 따라서 이에 대한 정보는 제공할 수 없습니다.
```

### 정보 접근 권한 있음

```
Question:  코카콜라의 제조 방법이 무엇입니까?
Context:  코카콜라의 제조 방법은 물, 설탕, 이산화탄소, 카페인, 인산, 향료 등을 섞어 만든다.
Answer: 코카콜라의 제조 방법은 물, 설탕, 이산화탄소, 카페인, 인산, 향료 등을 섞어 만드는 것으로 알려져 있습니다. 자세한 정보는 해당 문서를 참조하십시오.
```

---

# 4. 답변 신뢰도 강화

- 해당 답변을 생성한 근거를 함께 제시하여 추가 설명이나 세부 정보가 필요한 경우 문서 및 원본 데이터를 직접 참조할 수 있음.

---

# 코난 LLM의 활용 범위

1. 업무문서 / 보고서 초안 생성
2. 계약서 검토
3. 자연어 인터페이스
4. 고객 맞춤 상담
5. 고객 상담 후처리 자동화
6. 고객 마케팅 활용
7. 지속적 정보 수집과 실시간 대응방안 초안 생성

---

# 결론

- 코난 LLM 개발을 통해 상대적으로 소규모의 기업 또한 충분한 데이터셋이 확보되어 있고, 자원을 집중할 시 좋은 품질의 LLM을 만들 수 있음을 확인.
- 전문 영역에서의 학습 데이터의 부재로 인해 LLM 활용이 어려웠던 분야를 위한 특화된 LLM 개발을 제공 가능함.
