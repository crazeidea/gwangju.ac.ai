---
marp: true
---
![bg right:30%](https://i0.wp.com/bdtechtalks.com/wp-content/uploads/2016/08/8384110298_b0bc7d6435_o.jpg?resize=696%2C522&ssl=1)
# **코난 LLM: 한국어 대규모 언어 모델**
### 양승현 • 도원철 • 오창민 • 김정태 | 코난테크놀로지
<br />
 2023468101 최강훈

---
# 개발배경
- 기업에서 상용 / 오픈소스 LLM을 활용하려는 과정에서 발생하는 문제점
  - 기업 내부 데이터의 외부 유출 우려
  - 학습과 추롱늘 위한 GPU 구입 등의 과다한 비용
  - 모델의 한계로 인한 환각 문제
- 이를 해결하기 위해 코난 LLM을 개발
  - 기업 내부에 설치 및 운영하여 데이터 유출을 원천 차단
  - 적절한 비용과 최적의 성능으로 효율적 운영
  - 벡터 검색을 통한 답변의 증강과 근거 제시가 가능
---
# 모델 학습
- 코난 LLM 13B 모델은 **H100 X 8 서버 8대**를 이용해 **23일간** 사전 학습을 진행
- AIHUB, 위키피디아 등의 공개 데이터, 뉴스, Github 등의 구입 데이터
- 코난 LLM 13B 모델의 학습 데이터 토큰수는 **5,395억**개
  - 유사한 파라미터 크기를 가진 Polyglot-ko 12.8B와 비교했을때 약 3배이상의 학습 데이터를 이용해 학습
  - 한국어 학습 토큰은 **3,318억**개로 GPT-3.5의 5.7억개, LLaMA 2의 12억개와 비교할 수 없을 만큼 많은 양을 학습
  
---